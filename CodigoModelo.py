# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ml5F4U1k4ncXMsMrqsOCP8q119GiE-we
"""

import tensorflow as tf
from tensorflow.keras import layers, models
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt
from IPython.display import clear_output

# Descargar y cargar el dataset 'tf_flowers'
(train_ds, dataset_info) = tfds.load(
    'tf_flowers',
    split='train',
    as_supervised=True,
    with_info=True
)

# Tamaño de imagen y batch
TAMAÑO_IMAGEN = 128
BATCH_TAMAÑO = 32

# Definir las clases
nombre_clases = dataset_info.features['label'].names
n_classes = len(nombre_clases)

# Redimensionar y escalar las imágenes
resize_rescale = tf.keras.Sequential([
    layers.Resizing(TAMAÑO_IMAGEN, TAMAÑO_IMAGEN),
    layers.Rescaling(1./255)
])

# Aumentación de datos (no se incluirá en el modelo, sino en el dataset)
data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal_and_vertical"),
    layers.RandomRotation(0.2)
])

# Aplicar procesamiento y preparación del dataset
train_ds = train_ds.map(lambda x, y: (resize_rescale(x), y))
train_ds = train_ds.shuffle(1000).batch(BATCH_TAMAÑO)

# Dividir el dataset en entrenamiento y validación
val_size = int(0.2 * tf.data.experimental.cardinality(train_ds).numpy())
val_ds = train_ds.take(val_size)
train_ds = train_ds.skip(val_size)

# Aplicar aumento de datos solo en el conjunto de entrenamiento
train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))

# Prefetch para mejorar el rendimiento de los datasets
train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)
val_ds = val_ds.prefetch(buffer_size=tf.data.AUTOTUNE)

# Definir el modelo (sin la capa de data_augmentation)
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(TAMAÑO_IMAGEN, TAMAÑO_IMAGEN, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(n_classes, activation='softmax')
])

# Compilar el modelo
model.compile(
    optimizer='adam',
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    metrics=['accuracy']
)

# Callback para visualizar en tiempo real
class RealTimePlotCallback(tf.keras.callbacks.Callback):
    def on_train_begin(self, logs=None):
        self.history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}
        plt.ion()

    def on_epoch_end(self, epoch, logs=None):
        self.history['loss'].append(logs['loss'])
        self.history['val_loss'].append(logs['val_loss'])
        self.history['accuracy'].append(logs['accuracy'])
        self.history['val_accuracy'].append(logs['val_accuracy'])

        clear_output(wait=True)
        plt.figure(figsize=(12, 5))

        # Graficar pérdida
        plt.subplot(1, 2, 1)
        plt.plot(self.history['loss'], label='Entrenamiento')
        plt.plot(self.history['val_loss'], label='Validación')
        plt.title('Pérdida')
        plt.xlabel('Épocas')
        plt.ylabel('Pérdida')
        plt.legend()

        # Graficar precisión
        plt.subplot(1, 2, 2)
        plt.plot(self.history['accuracy'], label='Entrenamiento')
        plt.plot(self.history['val_accuracy'], label='Validación')
        plt.title('Precisión')
        plt.xlabel('Épocas')
        plt.ylabel('Precisión')
        plt.legend()

        plt.show()
        plt.pause(0.1)

# Entrenar el modelo usando el callback de gráficos en tiempo real
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=70,
    verbose=2,
    callbacks=[RealTimePlotCallback()]
)

!pip install tensorflowjs
model.save('mi_modelo.h5')
!tensorflowjs_converter --input_format keras mi_modelo.h5 carpeta_modelo_js